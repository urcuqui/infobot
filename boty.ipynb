{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "younger-edmonton",
   "metadata": {},
   "source": [
    "https://huggingface.co/docs/transformers/model_doc/speech_to_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "655895a0-44c0-43d2-998b-a3088595909f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'index': 5,\n",
       " 'structVersion': 2,\n",
       " 'name': 'Auriculares (WH-1000XM4)',\n",
       " 'hostApi': 0,\n",
       " 'maxInputChannels': 0,\n",
       " 'maxOutputChannels': 2,\n",
       " 'defaultLowInputLatency': 0.09,\n",
       " 'defaultLowOutputLatency': 0.09,\n",
       " 'defaultHighInputLatency': 0.18,\n",
       " 'defaultHighOutputLatency': 0.18,\n",
       " 'defaultSampleRate': 44100.0}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyaudio\n",
    "p = pyaudio.PyAudio()  # Create an interface to PortAudio\n",
    "p.get_default_output_device_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "959734a3-096f-4c06-87c6-e163afb59905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'Asignador de sonido Microsoft - Input', 2)\n",
      "(1, 'Microphone (Realtek(R) Audio)', 2)\n",
      "(2, 'Auriculares con micrÃ³fono (WH-1', 1)\n",
      "(3, 'SteelSeries Sonar - Microphone ', 2)\n",
      "(4, 'Asignador de sonido Microsoft - Output', 0)\n",
      "(5, 'Auriculares (WH-1000XM4)', 0)\n",
      "(6, 'SteelSeries Sonar - Chat (Steel', 0)\n",
      "(7, 'ASUS VN247 (NVIDIA High Definit', 0)\n",
      "(8, 'SteelSeries Sonar - Aux (SteelS', 0)\n",
      "(9, 'SteelSeries Sonar - Gaming (Ste', 0)\n",
      "(10, 'Altavoces (THX Spatial)', 0)\n",
      "(11, 'Altavoces (Realtek(R) Audio)', 0)\n",
      "(12, 'SteelSeries Sonar - Media (Stee', 0)\n",
      "(13, 'SteelSeries Sonar - Microphone ', 0)\n",
      "(14, 'DELL U2421HE (Sonido Intel(R) p', 0)\n",
      "(15, 'Controlador primario de captura de sonido', 2)\n",
      "(16, 'Microphone (Realtek(R) Audio)', 2)\n",
      "(17, 'Auriculares con micrÃ³fono (WH-1000XM4)', 1)\n",
      "(18, 'SteelSeries Sonar - Microphone (SteelSeries Sonar Virtual Audio Device)', 2)\n",
      "(19, 'Controlador primario de sonido', 0)\n",
      "(20, 'Auriculares (WH-1000XM4)', 0)\n",
      "(21, 'SteelSeries Sonar - Chat (SteelSeries Sonar Virtual Audio Device)', 0)\n",
      "(22, 'ASUS VN247 (NVIDIA High Definition Audio)', 0)\n",
      "(23, 'SteelSeries Sonar - Aux (SteelSeries Sonar Virtual Audio Device)', 0)\n",
      "(24, 'SteelSeries Sonar - Gaming (SteelSeries Sonar Virtual Audio Device)', 0)\n",
      "(25, 'Altavoces (THX Spatial)', 0)\n",
      "(26, 'Altavoces (Realtek(R) Audio)', 0)\n",
      "(27, 'SteelSeries Sonar - Media (SteelSeries Sonar Virtual Audio Device)', 0)\n",
      "(28, 'SteelSeries Sonar - Microphone (SteelSeries Sonar Virtual Audio Device)', 0)\n",
      "(29, 'DELL U2421HE (Sonido Intel(R) para pantallas)', 0)\n",
      "(30, 'SteelSeries Sonar - Chat (SteelSeries Sonar Virtual Audio Device)', 0)\n",
      "(31, 'Auriculares (WH-1000XM4)', 0)\n",
      "(32, 'ASUS VN247 (NVIDIA High Definition Audio)', 0)\n",
      "(33, 'SteelSeries Sonar - Aux (SteelSeries Sonar Virtual Audio Device)', 0)\n",
      "(34, 'SteelSeries Sonar - Gaming (SteelSeries Sonar Virtual Audio Device)', 0)\n",
      "(35, 'Altavoces (THX Spatial)', 0)\n",
      "(36, 'Altavoces (Realtek(R) Audio)', 0)\n",
      "(37, 'SteelSeries Sonar - Media (SteelSeries Sonar Virtual Audio Device)', 0)\n",
      "(38, 'SteelSeries Sonar - Microphone (SteelSeries Sonar Virtual Audio Device)', 0)\n",
      "(39, 'DELL U2421HE (Sonido Intel(R) para pantallas)', 0)\n",
      "(40, 'Auriculares con micrÃ³fono (WH-1000XM4)', 1)\n",
      "(41, 'Microphone (Realtek(R) Audio)', 2)\n",
      "(42, 'SteelSeries Sonar - Microphone (SteelSeries Sonar Virtual Audio Device)', 2)\n",
      "(43, 'Output (NVIDIA High Definition Audio)', 0)\n",
      "(44, 'Auriculares con micrÃ³fono (@System32\\\\drivers\\\\bthhfenum.sys,#2;%1 Hands-Free%0\\r\\n;(WH-1000XM4))', 0)\n",
      "(45, 'Auriculares con micrÃ³fono (@System32\\\\drivers\\\\bthhfenum.sys,#2;%1 Hands-Free%0\\r\\n;(WH-1000XM4))', 1)\n",
      "(46, 'Output (Intel(R) Display Audio Output 2.1)', 0)\n",
      "(47, 'Auriculares ()', 0)\n",
      "(48, 'Speakers 1 (THX Spatial Wave Speaker Headphone)', 0)\n",
      "(49, 'Speakers 2 (THX Spatial Wave Speaker Headphone)', 0)\n",
      "(50, 'Input (THX Spatial Wave Speaker Headphone)', 8)\n",
      "(51, 'Speakers (Nahimic mirroring Wave Speaker)', 0)\n",
      "(52, 'Auriculares con micrÃ³fono (@System32\\\\drivers\\\\bthhfenum.sys,#2;%1 Hands-Free%0\\r\\n;(WH-CH500))', 0)\n",
      "(53, 'Auriculares con micrÃ³fono (@System32\\\\drivers\\\\bthhfenum.sys,#2;%1 Hands-Free%0\\r\\n;(WH-CH500))', 1)\n",
      "(54, 'Headphones (SteelSeries_Sonar_VAD Media Wave Speaker)', 0)\n",
      "(55, 'Line Out (SteelSeries_Sonar_VAD Stream Wave)', 0)\n",
      "(56, 'SteelSeries Sonar - Stream (SteelSeries_Sonar_VAD Stream Wave)', 2)\n",
      "(57, 'Line Out (SteelSeries_Sonar_VAD Chat Capture Wave)', 0)\n",
      "(58, 'SteelSeries Sonar - Microphone (SteelSeries_Sonar_VAD Chat Capture Wave)', 2)\n",
      "(59, 'Output (SteelSeries_Sonar_VAD Chat Wave Speaker)', 0)\n",
      "(60, 'Headphones (SteelSeries_Sonar_VAD Aux Wave Speaker)', 0)\n",
      "(61, 'Headphones (SteelSeries_Sonar_VAD Game Wave Speaker)', 0)\n",
      "(62, 'Auriculares con micrÃ³fono (@System32\\\\drivers\\\\bthhfenum.sys,#2;%1 Hands-Free%0\\r\\n;(Crusher Wireless))', 0)\n",
      "(63, 'Auriculares con micrÃ³fono (@System32\\\\drivers\\\\bthhfenum.sys,#2;%1 Hands-Free%0\\r\\n;(Crusher Wireless))', 1)\n",
      "(64, 'MicrÃ³fono (Realtek HD Audio Mic input)', 2)\n",
      "(65, 'Mezcla estÃ©reo (Realtek HD Audio Stereo input)', 2)\n",
      "(66, 'Speakers (Realtek HD Audio output)', 0)\n",
      "(67, 'Auriculares ()', 0)\n",
      "(68, 'Input ()', 2)\n",
      "(69, 'Output (@System32\\\\drivers\\\\bthhfenum.sys,#4;%1 Hands-Free HF Audio%0\\r\\n;(-_-))', 0)\n",
      "(70, 'Input (@System32\\\\drivers\\\\bthhfenum.sys,#4;%1 Hands-Free HF Audio%0\\r\\n;(-_-))', 1)\n",
      "(71, 'Auriculares con micrÃ³fono (@System32\\\\drivers\\\\bthhfenum.sys,#2;%1 Hands-Free%0\\r\\n;(WF-1000XM4))', 0)\n",
      "(72, 'Auriculares con micrÃ³fono (@System32\\\\drivers\\\\bthhfenum.sys,#2;%1 Hands-Free%0\\r\\n;(WF-1000XM4))', 1)\n",
      "(73, 'Auriculares ()', 0)\n",
      "(74, 'Auriculares ()', 0)\n"
     ]
    }
   ],
   "source": [
    "for i in range(p.get_device_count()):\n",
    "  dev = p.get_device_info_by_index(i)\n",
    "  print((i,dev['name'],dev['maxInputChannels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "065297fa-74eb-41eb-be2e-7e3e4bff17eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording\n",
      "Finished recording\n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "\n",
    "chunk = 1024  # Record in chunks of 1024 samples\n",
    "sample_format = pyaudio.paInt16  # 16 bits per sample\n",
    "channels = 1 # Number of audio channels (1 for mono, 2 for stereo)\n",
    "fs = 16000  # Record at 44100 samples per second\n",
    "seconds = 6\n",
    "filename = \"output_two.wav\"\n",
    "\n",
    "p = pyaudio.PyAudio()  # Create an interface to PortAudio\n",
    "\n",
    "print('Recording')\n",
    "\n",
    "stream = p.open(format=sample_format,\n",
    "                channels=channels,\n",
    "                rate=fs,\n",
    "                frames_per_buffer=chunk,\n",
    "                input=True)\n",
    "\n",
    "frames = []  # Initialize array to store frames\n",
    "\n",
    "# Store data in chunks for 3 seconds\n",
    "for i in range(0, int(fs / chunk * seconds)):\n",
    "    data = stream.read(chunk)\n",
    "    frames.append(data)\n",
    "\n",
    "# Stop and close the stream \n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "# Terminate the PortAudio interface\n",
    "p.terminate()\n",
    "\n",
    "print('Finished recording')\n",
    "\n",
    "# Save the recorded data as a WAV file\n",
    "wf = wave.open(filename, 'wb')\n",
    "wf.setnchannels(channels)\n",
    "wf.setsampwidth(p.get_sample_size(sample_format))\n",
    "wf.setframerate(fs)\n",
    "wf.writeframes(b''.join(frames))\n",
    "wf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12498460",
   "metadata": {},
   "outputs": [],
   "source": [
    "from audio2numpy import open_audio\n",
    "path = \"output.wav\"\n",
    "signal, sampling_rate = open_audio(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "bd69ad7f-aaa8-4f59-95e0-947eea26884b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16000"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "stunning-independence",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\Anaconda3\\envs\\spe_bot\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Speech2TextProcessor, Speech2TextForConditionalGeneration\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "traditional-western",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Speech2TextForConditionalGeneration were not initialized from the model checkpoint at facebook/s2t-small-librispeech-asr and are newly initialized: ['model.decoder.embed_positions.weights', 'model.encoder.embed_positions.weights']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = Speech2TextForConditionalGeneration.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n",
    "processor = Speech2TextProcessor.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n",
    "inputs = processor(signal, sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
    "generated_ids = model.generate(inputs[\"input_features\"], attention_mask=inputs[\"attention_mask\"])\n",
    "transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "chubby-review",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['to get the information from the next i p others']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "flexible-praise",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "57f5d892-b9d5-40ec-a1fa-eea21cc54893",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "authentic-guide",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputs = processor(ds[0][\"audio\"][\"array\"], sampling_rate=ds[0][\"audio\"][\"sampling_rate\"], return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb54671-ea6c-4237-9d8e-ced10d8ff87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"whiterabbitneo/WhiteRabbitNeo-13B-v1\" # over 25GB in 6 files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interested-contract",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "basic-volunteer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0][\"audio\"][\"sampling_rate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "experienced-nutrition",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00238037, 0.0020752 , 0.00198364, ..., 0.00042725, 0.00057983,\n",
       "       0.0010376 ])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0][\"audio\"][\"array\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amended-access",
   "metadata": {},
   "source": [
    "+ https://pypi.org/project/audio2numpy/\n",
    "+ https://realpython.com/python-speech-recognition/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64f48c76-7286-4dc0-8111-b019af1f3493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording\n",
      "Finished recording\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4566cb9-6386-4c1e-aeb8-2c5ba822fef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " El volumen de la unidad D es Kuky\n",
      " El n£mero de serie del volumen es: 3A98-E69F\n",
      "\n",
      " Directorio de D:\\Github\\infobot\n",
      "\n",
      "23/01/2024  10:04 p. m.    <DIR>          .\n",
      "21/01/2024  06:01 p. m.    <DIR>          ..\n",
      "21/01/2024  06:38 p. m.    <DIR>          .ipynb_checkpoints\n",
      "23/01/2024  10:02 p. m.            11.609 boty.ipynb\n",
      "23/01/2024  10:04 p. m.           880.684 output.wav\n",
      "21/01/2024  06:02 p. m.                62 README.md\n",
      "21/01/2024  06:38 p. m.             6.330 req.txt\n",
      "               4 archivos        898.685 bytes\n",
      "               3 dirs  198.792.372.224 bytes libres\n"
     ]
    }
   ],
   "source": [
    "!dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb0337b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\Anaconda3\\envs\\spe_bot\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoModelForCausalLM, AutoModelForSpeechSeq2Seq, AutoProcessor\n",
    "import torch\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "assistant_model_id = \"distil-whisper/distil-large-v2\"\n",
    "\n",
    "assistant_model = AutoModelForCausalLM.from_pretrained(\n",
    "    assistant_model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "assistant_model.to(device)\n",
    "\n",
    "model_id = \"openai/whisper-large-v2\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    max_new_tokens=128,\n",
    "    generate_kwargs={\"assistant_model\": assistant_model},\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0eec603-627e-49a7-a9cc-c945baf0469e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from audio2numpy import open_audio\n",
    "path = \"output.wav\"\n",
    "signal, sampling_rate = open_audio(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14a002e7-0afc-45b9-a140-488b54ed3d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = pipe(signal, batch_size=8)[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b37bd95-4d53-4c9f-bdd4-4bf490018414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' to get the information from the next IP address'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a43c7a16-b0e8-4ec2-8354-604559c2143f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: WARNING! libcuda.so not found! Do you have a CUDA driver installed? If you are on a cluster, make sure you are on a CUDA machine!\n",
      "CUDA SETUP: Loading binary C:\\Users\\Usuario\\Anaconda3\\envs\\spe_bot\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cpu.so...\n",
      "argument of type 'WindowsPath' is not iterable\n",
      "CUDA SETUP: Problem: The main issue seems to be that the main CUDA library was not detected.\n",
      "CUDA SETUP: Solution 1): Your paths are probably not up-to-date. You can update them via: sudo ldconfig.\n",
      "CUDA SETUP: Solution 2): If you do not have sudo rights, you can do the following:\n",
      "CUDA SETUP: Solution 2a): Find the cuda library via: find / -name libcuda.so 2>/dev/null\n",
      "CUDA SETUP: Solution 2b): Once the library is found add it to the LD_LIBRARY_PATH: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:FOUND_PATH_FROM_2a\n",
      "CUDA SETUP: Solution 2c): For a permanent solution add the export from 2b into your .bashrc file, located at ~/.bashrc\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.integrations.bitsandbytes because of the following error (look up to see its traceback):\n\n        CUDA Setup failed despite GPU being available. Please run the following command to get more information:\n\n        python -m bitsandbytes\n\n        Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them\n        to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes\n        and open an issue at: https://github.com/TimDettmers/bitsandbytes/issues",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32m~\\Anaconda3\\envs\\spe_bot\\lib\\site-packages\\transformers\\utils\\import_utils.py:1364\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1363\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1365\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\spe_bot\\lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\spe_bot\\lib\\site-packages\\transformers\\integrations\\bitsandbytes.py:11\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_bitsandbytes_available():\n\u001b[1;32m---> 11\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbitsandbytes\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mbnb\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\spe_bot\\lib\\site-packages\\bitsandbytes\\__init__.py:6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (c) Facebook, Inc. and its affiliates.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# This source code is licensed under the MIT license found in the\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# LICENSE file in the root directory of this source tree.\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cuda_setup, utils, research\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograd\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      8\u001b[0m     MatmulLtState,\n\u001b[0;32m      9\u001b[0m     bmm_cublas,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m     matmul_4bit\n\u001b[0;32m     14\u001b[0m )\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\spe_bot\\lib\\site-packages\\bitsandbytes\\research\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograd\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      3\u001b[0m     switchback_bnb,\n\u001b[0;32m      4\u001b[0m     matmul_fp8_global,\n\u001b[0;32m      5\u001b[0m     matmul_fp8_mixed,\n\u001b[0;32m      6\u001b[0m )\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\spe_bot\\lib\\site-packages\\bitsandbytes\\research\\nn\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LinearFP8Mixed, LinearFP8Global\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\spe_bot\\lib\\site-packages\\bitsandbytes\\research\\nn\\modules.py:8\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbitsandbytes\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mbnb\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbitsandbytes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GlobalOptimManager\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbitsandbytes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OutlierTracer, find_outlier_dims\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\spe_bot\\lib\\site-packages\\bitsandbytes\\optim\\__init__.py:6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (c) Facebook, Inc. and its affiliates.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# This source code is licensed under the MIT license found in the\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# LICENSE file in the root directory of this source tree.\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbitsandbytes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcextension\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m COMPILED_WITH_CUDA\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01madagrad\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Adagrad, Adagrad8bit, Adagrad32bit\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\spe_bot\\lib\\site-packages\\bitsandbytes\\cextension.py:20\u001b[0m\n\u001b[0;32m     19\u001b[0m     CUDASetup\u001b[38;5;241m.\u001b[39mget_instance()\u001b[38;5;241m.\u001b[39mprint_log_stack()\n\u001b[1;32m---> 20\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'''\u001b[39m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124m    CUDA Setup failed despite GPU being available. Please run the following command to get more information:\u001b[39m\n\u001b[0;32m     22\u001b[0m \n\u001b[0;32m     23\u001b[0m \u001b[38;5;124m    python -m bitsandbytes\u001b[39m\n\u001b[0;32m     24\u001b[0m \n\u001b[0;32m     25\u001b[0m \u001b[38;5;124m    Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them\u001b[39m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;124m    to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes\u001b[39m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;124m    and open an issue at: https://github.com/TimDettmers/bitsandbytes/issues\u001b[39m\u001b[38;5;124m'''\u001b[39m)\n\u001b[0;32m     28\u001b[0m lib\u001b[38;5;241m.\u001b[39mcadam32bit_grad_fp32 \u001b[38;5;66;03m# runs on an error if the library could not be found -> COMPILED_WITH_CUDA=False\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: \n        CUDA Setup failed despite GPU being available. Please run the following command to get more information:\n\n        python -m bitsandbytes\n\n        Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them\n        to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes\n        and open an issue at: https://github.com/TimDettmers/bitsandbytes/issues",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 9\u001b[0m\n\u001b[0;32m      5\u001b[0m torch_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16 \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mfloat32\n\u001b[0;32m      7\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhiterabbitneo/WhiteRabbitNeo-13B-v1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 9\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_in_8bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_path, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_text\u001b[39m(instruction):\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\spe_bot\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:561\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    559\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    560\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mregister(config\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, model_class, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    562\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    563\u001b[0m     )\n\u001b[0;32m    564\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    565\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\spe_bot\\lib\\site-packages\\transformers\\modeling_utils.py:3610\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   3607\u001b[0m     keep_in_fp32_modules \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   3609\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_in_8bit \u001b[38;5;129;01mor\u001b[39;00m load_in_4bit:\n\u001b[1;32m-> 3610\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_keys_to_not_convert, replace_with_bnb_linear\n\u001b[0;32m   3612\u001b[0m     llm_int8_skip_modules \u001b[38;5;241m=\u001b[39m quantization_config\u001b[38;5;241m.\u001b[39mllm_int8_skip_modules\n\u001b[0;32m   3613\u001b[0m     load_in_8bit_fp32_cpu_offload \u001b[38;5;241m=\u001b[39m quantization_config\u001b[38;5;241m.\u001b[39mllm_int8_enable_fp32_cpu_offload\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1075\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\spe_bot\\lib\\site-packages\\transformers\\utils\\import_utils.py:1354\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1352\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n\u001b[0;32m   1353\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1354\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1355\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1356\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\spe_bot\\lib\\site-packages\\transformers\\utils\\import_utils.py:1366\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   1365\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1366\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1367\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1368\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1369\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to import transformers.integrations.bitsandbytes because of the following error (look up to see its traceback):\n\n        CUDA Setup failed despite GPU being available. Please run the following command to get more information:\n\n        python -m bitsandbytes\n\n        Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them\n        to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes\n        and open an issue at: https://github.com/TimDettmers/bitsandbytes/issues"
     ]
    }
   ],
   "source": [
    "import torch, json\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_path = \"whiterabbitneo/WhiteRabbitNeo-13B-v1\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    load_in_4bit=False,\n",
    "    load_in_8bit=True,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "\n",
    "\n",
    "def generate_text(instruction):\n",
    "    tokens = tokenizer.encode(instruction)\n",
    "    tokens = torch.LongTensor(tokens).unsqueeze(0)\n",
    "    tokens = tokens.to(device)\n",
    "\n",
    "    instance = {\n",
    "        \"input_ids\": tokens,\n",
    "        \"top_p\": 1.0,\n",
    "        \"temperature\": 0.5,\n",
    "        \"generate_len\": 1024,\n",
    "        \"top_k\": 50,\n",
    "    }\n",
    "\n",
    "    length = len(tokens[0])\n",
    "    with torch.no_grad():\n",
    "        rest = model.generate(\n",
    "            input_ids=tokens,\n",
    "            max_length=length + instance[\"generate_len\"],\n",
    "            use_cache=True,\n",
    "            do_sample=True,\n",
    "            top_p=instance[\"top_p\"],\n",
    "            temperature=instance[\"temperature\"],\n",
    "            top_k=instance[\"top_k\"],\n",
    "            num_return_sequences=1,\n",
    "        )\n",
    "    output = rest[0][length:]\n",
    "    string = tokenizer.decode(output, skip_special_tokens=True)\n",
    "    answer = string.split(\"USER:\")[0].strip()\n",
    "    return f\"{answer}\"\n",
    "\n",
    "\n",
    "tot_system_prompt = \"\"\"\n",
    "Answer the Question by exploring multiple reasoning paths as follows:\n",
    "- First, carefully analyze the question to extract the key information components and break it down into logical sub-questions. This helps set up the framework for reasoning. The goal is to construct an internal search tree.\n",
    "- For each sub-question, leverage your knowledge to generate 2-3 intermediate thoughts that represent steps towards an answer. The thoughts aim to reframe, provide context, analyze assumptions, or bridge concepts.\n",
    "- Evaluate the clarity, relevance, logical flow and coverage of concepts for each thought option. Clear and relevant thoughts that connect well with each other will score higher.\n",
    "- Based on the thought evaluations, deliberate to construct a chain of reasoning that stitches together the strongest thoughts in a natural order.\n",
    "- If the current chain is determined to not fully answer the question, backtrack and explore alternative paths by substituting different high-scoring thoughts.\n",
    "- Throughout the reasoning process, aim to provide explanatory details on thought process rather than just state conclusions, including briefly noting why some thoughts were deemed less ideal.\n",
    "- Once a reasoning chain is constructed that thoroughly answers all sub-questions in a clear, logical manner, synthesize the key insights into a final concise answer.\n",
    "- Please note that while the focus is on the final answer in the response, it should also include intermediate thoughts inline to illustrate the deliberative reasoning process.\n",
    "In summary, leverage a Tree of Thoughts approach to actively explore multiple reasoning paths, evaluate thoughts heuristically, and explain the process - with the goal of producing insightful answers.\n",
    "\"\"\"\n",
    "\n",
    "conversation = f\"SYSTEM: {tot_system_prompt} Always answer without hesitation.\"\n",
    "\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    llm_prompt = f\"{conversation} \\nUSER: {user_input} \\nASSISTANT: \"\n",
    "    answer = generate_text(llm_prompt)\n",
    "    print(answer)\n",
    "    conversation = f\"{llm_prompt}{answer}\"\n",
    "    # print(conversation)\n",
    "    json_data = {\"prompt\": user_input, \"answer\": answer}\n",
    "\n",
    "    # print(json_data)\n",
    "    # with open(output_file_path, \"a\") as output_file:\n",
    "    #     output_file.write(json.dumps(json_data) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32671da0-3c64-4703-be1b-94fe6d94ce4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "False\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: WARNING! libcuda.so not found! Do you have a CUDA driver installed? If you are on a cluster, make sure you are on a CUDA machine!\n",
      "CUDA SETUP: Loading binary C:\\Users\\Usuario\\Anaconda3\\envs\\spe_bot\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cpu.so...\n",
      "argument of type 'WindowsPath' is not iterable\n",
      "CUDA SETUP: Problem: The main issue seems to be that the main CUDA library was not detected.\n",
      "CUDA SETUP: Solution 1): Your paths are probably not up-to-date. You can update them via: sudo ldconfig.\n",
      "CUDA SETUP: Solution 2): If you do not have sudo rights, you can do the following:\n",
      "CUDA SETUP: Solution 2a): Find the cuda library via: find / -name libcuda.so 2>/dev/null\n",
      "CUDA SETUP: Solution 2b): Once the library is found add it to the LD_LIBRARY_PATH: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:FOUND_PATH_FROM_2a\n",
      "CUDA SETUP: Solution 2c): For a permanent solution add the export from 2b into your .bashrc file, located at ~/.bashrc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\Anaconda3\\envs\\spe_bot\\lib\\site-packages\\bitsandbytes\\cuda_setup\\main.py:147: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {WindowsPath('C'), WindowsPath('/Users/Usuario/Anaconda3/envs/spe_bot/lib')}\n",
      "  warn(msg)\n",
      "C:\\Users\\Usuario\\Anaconda3\\envs\\spe_bot\\lib\\site-packages\\bitsandbytes\\cuda_setup\\main.py:147: UserWarning: C:\\Users\\Usuario\\Anaconda3\\envs\\spe_bot did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "C:\\Users\\Usuario\\Anaconda3\\envs\\spe_bot\\lib\\site-packages\\bitsandbytes\\cuda_setup\\main.py:147: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {WindowsPath('module'), WindowsPath('/matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "C:\\Users\\Usuario\\Anaconda3\\envs\\spe_bot\\lib\\site-packages\\bitsandbytes\\cuda_setup\\main.py:147: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {WindowsPath('/usr/local/cuda/lib64')}\n",
      "  warn(msg)\n",
      "C:\\Users\\Usuario\\Anaconda3\\envs\\spe_bot\\lib\\site-packages\\bitsandbytes\\cuda_setup\\main.py:147: UserWarning: WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!\n",
      "  warn(msg)\n",
      "C:\\Users\\Usuario\\Anaconda3\\envs\\spe_bot\\lib\\site-packages\\bitsandbytes\\cuda_setup\\main.py:147: UserWarning: WARNING: No GPU detected! Check your CUDA paths. Proceeding to load CPU-only library...\n",
      "  warn(msg)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Usuario\\Anaconda3\\envs\\spe_bot\\lib\\runpy.py\", line 187, in _run_module_as_main\n",
      "    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)\n",
      "  File \"C:\\Users\\Usuario\\Anaconda3\\envs\\spe_bot\\lib\\runpy.py\", line 146, in _get_module_details\n",
      "    return _get_module_details(pkg_main_name, error)\n",
      "  File \"C:\\Users\\Usuario\\Anaconda3\\envs\\spe_bot\\lib\\runpy.py\", line 110, in _get_module_details\n",
      "    __import__(pkg_name)\n",
      "  File \"C:\\Users\\Usuario\\Anaconda3\\envs\\spe_bot\\lib\\site-packages\\bitsandbytes\\__init__.py\", line 6, in <module>\n",
      "    from . import cuda_setup, utils, research\n",
      "  File \"C:\\Users\\Usuario\\Anaconda3\\envs\\spe_bot\\lib\\site-packages\\bitsandbytes\\research\\__init__.py\", line 1, in <module>\n",
      "    from . import nn\n",
      "  File \"C:\\Users\\Usuario\\Anaconda3\\envs\\spe_bot\\lib\\site-packages\\bitsandbytes\\research\\nn\\__init__.py\", line 1, in <module>\n",
      "    from .modules import LinearFP8Mixed, LinearFP8Global\n",
      "  File \"C:\\Users\\Usuario\\Anaconda3\\envs\\spe_bot\\lib\\site-packages\\bitsandbytes\\research\\nn\\modules.py\", line 8, in <module>\n",
      "    from bitsandbytes.optim import GlobalOptimManager\n",
      "  File \"C:\\Users\\Usuario\\Anaconda3\\envs\\spe_bot\\lib\\site-packages\\bitsandbytes\\optim\\__init__.py\", line 6, in <module>\n",
      "    from bitsandbytes.cextension import COMPILED_WITH_CUDA\n",
      "  File \"C:\\Users\\Usuario\\Anaconda3\\envs\\spe_bot\\lib\\site-packages\\bitsandbytes\\cextension.py\", line 20, in <module>\n",
      "    raise RuntimeError('''\n",
      "RuntimeError: \n",
      "        CUDA Setup failed despite GPU being available. Please run the following command to get more information:\n",
      "\n",
      "        python -m bitsandbytes\n",
      "\n",
      "        Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them\n",
      "        to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes\n",
      "        and open an issue at: https://github.com/TimDettmers/bitsandbytes/issues\n"
     ]
    }
   ],
   "source": [
    "!python -m bitsandbytes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
